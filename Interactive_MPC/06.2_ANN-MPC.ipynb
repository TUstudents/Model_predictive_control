{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7243861e",
      "metadata": {},
      "source": [
        "# Notebook 6.2: MPC with Artificial Neural Network (ANN) Models (ANN-MPC)\n",
        "\n",
        "In this notebook, we take our first step into implementing Model Predictive Control using a data-driven model. We will focus on **Artificial Neural Networks (ANNs)**, specifically Feedforward Neural Networks (FNNs), also known as Multi-Layer Perceptrons (MLPs).\n",
        "\n",
        "The process will involve:\n",
        "1.  Generating or loading a dataset from a (known for this example) nonlinear dynamic system.\n",
        "2.  Designing and training an ANN using **PyTorch** to learn the one-step-ahead state transition function: $\\hat{x}_{k+1} = f_{ANN}(x_k, u_k)$.\n",
        "3.  Validating the performance of the trained ANN model.\n",
        "4.  Integrating this PyTorch ANN model into an NMPC controller formulated with **CasADi**.\n",
        "5.  Simulating the closed-loop ANN-MPC system and evaluating its performance.\n",
        "\n",
        "**Goals of this Notebook:**\n",
        "-   Learn how to train an ANN to model discrete-time nonlinear dynamics.\n",
        "-   Understand methods for integrating a pre-trained PyTorch ANN into a CasADi-based NMPC optimization problem.\n",
        "-   Implement and simulate an ANN-MPC controller.\n",
        "-   Discuss the practical considerations and challenges of using ANNs in MPC."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e4135a",
      "metadata": {},
      "source": [
        "## 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a30eb4cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import casadi as ca\n",
        "\n",
        "# Optional: for nicer plots\n",
        "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 6)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ff7a57",
      "metadata": {},
      "source": [
        "## 2. The Nonlinear System (Plant) and Data Generation\n",
        "\n",
        "For this example, let's use a simple but nonlinear system: a **Continuous Stirred Tank Reactor (CSTR)** with a first-order irreversible reaction $A \\rightarrow B$. We'll use the model from Notebook 8.6 (if following the main course numbering, or a similar one).\n",
        "\n",
        "**CSTR Model (Continuous-Time):**\n",
        "$$ \\frac{dC_A}{dt} = \\frac{F}{V}(C_{A,in} - C_A) - k_0 e^{-E_a/(RT)} C_A $$\n",
        "$$ \\frac{dT}{dt} = \\frac{F}{V}(T_{in} - T) + \\frac{(-\\Delta H_R)}{\\rho C_p} k_0 e^{-E_a/(RT)} C_A - \\frac{UA_c}{\\rho C_p V}(T - T_c) $$\n",
        "States: $x = [C_A, T]^T$\n",
        "Input: $u = T_c$ (Coolant Temperature)\n",
        "\n",
        "We need to generate discrete-time data $(x_k, u_k) \\rightarrow x_{k+1}$ for training the ANN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d307f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSTR Parameters (example values)\n",
        "F_val = 1.0      # Volumetric flow rate (m^3/min)\n",
        "V_val = 1.0      # Reactor volume (m^3)\n",
        "CA_in_val = 1.0  # Inlet concentration of A (mol/m^3)\n",
        "T_in_val = 300.0 # Inlet temperature (K)\n",
        "k0_val = 7.2e10  # Pre-exponential factor (min^-1)\n",
        "Ea_R_val = 1e4   # Activation energy / Gas constant (K)\n",
        "delta_H_val = -5e4 # Enthalpy of reaction (J/mol)\n",
        "rho_Cp_val = 4.2e6 # Density * Specific heat (J/m^3/K)\n",
        "UA_val = 5e4     # Overall heat transfer coefficient * Area (J/min/K)\n",
        "Ts_data_gen = 0.5 # Sampling time for data generation (min)\n",
        "\n",
        "def cstr_ode_numpy(t, x, Tc_input, params):\n",
        "    Ca, T = x\n",
        "    F, V, CA_in, T_in, k0, Ea_R, delta_H, rho_Cp, UA = params\n",
        "    \n",
        "    k_reaction = k0 * np.exp(-Ea_R / T)\n",
        "    \n",
        "    dCa_dt = (F/V) * (CA_in - Ca) - k_reaction * Ca\n",
        "    dT_dt = (F/V) * (T_in - T) + (-delta_H / rho_Cp) * k_reaction * Ca - (UA / (rho_Cp * V)) * (T - Tc_input)\n",
        "    return [dCa_dt, dT_dt]\n",
        "\n",
        "cstr_params_list = [F_val, V_val, CA_in_val, T_in_val, k0_val, Ea_R_val, delta_H_val, rho_Cp_val, UA_val]\n",
        "\n",
        "# Generate Training/Validation/Test Data\n",
        "N_total_samples = 5000\n",
        "x_data = []\n",
        "u_data = []\n",
        "x_next_data = []\n",
        "\n",
        "x_current = np.array([0.5, 350.0]) # Initial state for data generation\n",
        "\n",
        "print(\"Generating CSTR data for ANN training...\")\n",
        "for i in range(N_total_samples):\n",
        "    # Random input Tc (coolant temperature) for excitation\n",
        "    u_current = np.random.uniform(280, 320) # K\n",
        "    \n",
        "    # Store current state and input\n",
        "    x_data.append(x_current.copy())\n",
        "    u_data.append(u_current)\n",
        "    \n",
        "    # Simulate one step forward\n",
        "    sol = solve_ivp(cstr_ode_numpy, [0, Ts_data_gen], x_current, \n",
        "                      args=(u_current, cstr_params_list), method='RK45')\n",
        "    x_next = sol.y[:, -1]\n",
        "    x_next_data.append(x_next.copy())\n",
        "    \n",
        "    # Update current state (with some small process noise for realism if desired)\n",
        "    x_current = x_next + np.random.normal(0, [0.001, 0.1], size=2) # Small process noise\n",
        "    x_current[0] = np.clip(x_current[0], 0.01, 1.0) # Keep Ca bounded\n",
        "    x_current[1] = np.clip(x_current[1], 280, 400) # Keep T bounded\n",
        "    if (i+1)%500 == 0: print(f\"  Generated {i+1}/{N_total_samples} samples...\", end='\\r')\n",
        "\n",
        "x_data = np.array(x_data)\n",
        "u_data = np.array(u_data).reshape(-1, 1)\n",
        "x_next_data = np.array(x_next_data)\n",
        "print(\"\\nData generation complete.\")\n",
        "print(f\"x_data shape: {x_data.shape}, u_data shape: {u_data.shape}, x_next_data shape: {x_next_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14c98f2",
      "metadata": {},
      "source": [
        "### Data Preprocessing\n",
        "Scaling inputs and outputs is crucial for good ANN training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c74648e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inputs to ANN: [Ca_k, T_k, Tc_k]\n",
        "ann_input_data = np.hstack((x_data, u_data))\n",
        "# Outputs of ANN: [Ca_k+1, T_k+1]\n",
        "ann_output_data = x_next_data\n",
        "\n",
        "# Scale data\n",
        "input_scaler = MinMaxScaler()\n",
        "output_scaler = MinMaxScaler()\n",
        "\n",
        "ann_input_scaled = input_scaler.fit_transform(ann_input_data)\n",
        "ann_output_scaled = output_scaler.fit_transform(ann_output_data)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    ann_input_scaled, ann_output_scaled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_torch = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_torch = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "print(f\"Training input shape: {X_train_torch.shape}\")\n",
        "print(f\"Training output shape: {y_train_torch.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c451c62e",
      "metadata": {},
      "source": [
        "## 3. Designing and Training the ANN Model (PyTorch)\n",
        "\n",
        "We'll define a simple Feedforward Neural Network (FNN/MLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98661a7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DynamicANN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(DynamicANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        # Output activation is linear for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "# ANN Hyperparameters\n",
        "input_dim_ann = ann_input_scaled.shape[1]   # Ca, T, Tc -> 3\n",
        "output_dim_ann = ann_output_scaled.shape[1] # Ca_next, T_next -> 2\n",
        "hidden1 = 32\n",
        "hidden2 = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200 # Adjust as needed\n",
        "batch_size = 64\n",
        "\n",
        "ann_model = DynamicANN(input_dim_ann, hidden1, hidden2, output_dim_ann)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(ann_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Training ANN model...\")\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ann_model.train()\n",
        "    epoch_train_loss = 0\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        outputs = ann_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss.item()\n",
        "    train_losses.append(epoch_train_loss / len(train_loader))\n",
        "    \n",
        "    ann_model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs_val, targets_val in val_loader:\n",
        "            outputs_val = ann_model(inputs_val)\n",
        "            loss_val = criterion(outputs_val, targets_val)\n",
        "            epoch_val_loss += loss_val.item()\n",
        "    val_losses.append(epoch_val_loss / len(val_loader))\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}')\n",
        "\n",
        "print(\"ANN training complete.\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('ANN Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39611b5a",
      "metadata": {},
      "source": [
        "## 4. Validating the Trained ANN Model\n",
        "\n",
        "Let's see how well the ANN predicts multi-step ahead (simulation) on a new sequence of inputs, compared to the true CSTR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c70e3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_test_sim = 200\n",
        "t_test_sim = np.arange(0, N_test_sim * Ts_data_gen, Ts_data_gen)\n",
        "\n",
        "# Generate a new test input sequence (e.g., steps or sinusoids)\n",
        "u_test = np.zeros(N_test_sim)\n",
        "u_test[0:50] = 290\n",
        "u_test[50:100] = 310\n",
        "u_test[100:150] = 285\n",
        "u_test[150:N_test_sim] = 300\n",
        "\n",
        "x_true_sim = np.zeros((N_test_sim + 1, 2))\n",
        "x_ann_sim = np.zeros((N_test_sim + 1, 2))\n",
        "\n",
        "x_true_sim[0, :] = np.array([0.6, 340.0]) # Initial condition for test\n",
        "x_ann_sim[0, :] = x_true_sim[0, :].copy()\n",
        "\n",
        "ann_model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for k in range(N_test_sim):\n",
        "        # True system step\n",
        "        sol_true = solve_ivp(cstr_ode_numpy, [0, Ts_data_gen], x_true_sim[k, :], \n",
        "                               args=(u_test[k], cstr_params_list), method='RK45')\n",
        "        x_true_sim[k+1, :] = sol_true.y[:, -1]\n",
        "        \n",
        "        # ANN prediction step (multi-step ahead)\n",
        "        current_ann_input_unscaled = np.hstack((x_ann_sim[k, :], u_test[k]))\n",
        "        current_ann_input_scaled = input_scaler.transform(current_ann_input_unscaled.reshape(1, -1))\n",
        "        current_ann_input_torch = torch.tensor(current_ann_input_scaled, dtype=torch.float32)\n",
        "        \n",
        "        predicted_ann_output_scaled = ann_model(current_ann_input_torch)\n",
        "        predicted_ann_output_unscaled = output_scaler.inverse_transform(predicted_ann_output_scaled.numpy())\n",
        "        x_ann_sim[k+1, :] = predicted_ann_output_unscaled.flatten()\n",
        "\n",
        "# Plot multi-step ahead prediction comparison\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(t_test_sim, x_true_sim[:-1, 0], 'b-', label='True $C_A$')\n",
        "plt.plot(t_test_sim, x_ann_sim[:-1, 0], 'r--', label='ANN Predicted $C_A$')\n",
        "plt.ylabel('$C_A$ (mol/m^3)'); plt.grid(True); plt.legend()\n",
        "plt.title('ANN Model Validation - Multi-Step Ahead Prediction')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(t_test_sim, x_true_sim[:-1, 1], 'b-', label='True T (K)')\n",
        "plt.plot(t_test_sim, x_ann_sim[:-1, 1], 'r--', label='ANN Predicted T (K)')\n",
        "plt.ylabel('Temperature T (K)'); plt.xlabel('Time (min)'); plt.grid(True); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3113b2b8",
      "metadata": {},
      "source": [
        "## 5. Integrating the PyTorch ANN into CasADi for NMPC\n",
        "\n",
        "To use our trained `ann_model` (PyTorch) within a CasADi optimization problem, we need a way for CasADi to call this model and, ideally, get its gradients. CasADi's `external` function or its support for importing ONNX models are common ways.\n",
        "\n",
        "**Method 1: Using CasADi's `external` function (conceptual, requires careful handling of PyTorch AD within CasADi's AD tape).**\n",
        "This can be complex if CasADi needs to differentiate *through* the PyTorch model. For NMPC, the NLP solver often needs derivatives of the objective/constraints with respect to the control inputs $\\mathbf{U}_k$. If the ANN is part of the prediction path $\\mathbf{Y}_k = f_{MPC}(\\mathbf{U}_k, x_k)$, then gradients of $f_{ANN}$ w.r.t its inputs are needed by CasADi's chain rule.\n",
        "\n",
        "**Method 2: Export PyTorch model to ONNX, then import ONNX into CasADi.**\n",
        "ONNX (Open Neural Network Exchange) is an open format for machine learning models. PyTorch can export to ONNX, and CasADi has capabilities to import ONNX models, which then become symbolic CasADi functions.\n",
        "\n",
        "**Method 3: Re-implementing the ANN in CasADi (for simple ANNs).**\n",
        "For simple FNNs, one could re-implement the network structure (linear layers, activations) directly using CasADi symbolic variables and use the trained PyTorch weights. This gives CasADi full symbolic access for AD.\n",
        "\n",
        "**For this notebook, we'll demonstrate Method 3 (Re-implementation) for simplicity, as it gives CasADi full symbolic control and easy AD.** For more complex ANNs or for using pre-built PyTorch models directly, ONNX export/import or careful use of `ca.Function.external` with custom derivative functions would be needed.\n",
        "\n",
        "*(Note: True black-box integration where CasADi differentiates through PyTorch's autograd tape directly is an advanced topic and might require specific CasADi plugins or careful setup not covered in this introductory notebook.)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f2be76",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-implement ANN structure in CasADi and load PyTorch weights\n",
        "# This assumes ann_model is our trained PyTorch model\n",
        "ann_model.eval() # Ensure it's in eval mode\n",
        "\n",
        "# Get weights and biases from PyTorch model\n",
        "W1 = ann_model.fc1.weight.data.numpy().T # Transpose for CasADi convention (features x neurons)\n",
        "b1 = ann_model.fc1.bias.data.numpy().reshape(-1, 1)\n",
        "W2 = ann_model.fc2.weight.data.numpy().T\n",
        "b2 = ann_model.fc2.bias.data.numpy().reshape(-1, 1)\n",
        "W3 = ann_model.fc3.weight.data.numpy().T\n",
        "b3 = ann_model.fc3.bias.data.numpy().reshape(-1, 1)\n",
        "\n",
        "def ann_casadi_model_scaled(scaled_input_sx, W1, b1, W2, b2, W3, b3):\n",
        "    # scaled_input_sx is CasADi SX (input_dim_ann x 1)\n",
        "    # Replicates the PyTorch model structure symbolically\n",
        "    layer1 = ca.relu(W1.T @ scaled_input_sx + b1) # Note: PyTorch Linear is xA^T + b\n",
        "                                                  # CasADi is typically A x + b for matrix A\n",
        "                                                  # So W1.T used here if W1 was features_out x features_in\n",
        "    # Our PyTorch W1 is (hidden_size1, input_size). For W1 @ x, W1 needs to be (hidden_size1, input_size)\n",
        "    # So W1.T in PyTorch is (input_size, hidden_size1). If x is (batch, input_size)\n",
        "    # (batch, input_size) @ (input_size, hidden_size1) -> (batch, hidden_size1)\n",
        "    # For CasADi: W1_cas (hidden_size1, input_size) @ x_cas (input_size, 1)\n",
        "    # PyTorch fc.weight is (out_features, in_features). So W1 shape is (hidden1, input_dim_ann)\n",
        "    layer1_cas = ca.relu(W1 @ scaled_input_sx + b1)\n",
        "    layer2_cas = ca.relu(W2 @ layer1_cas + b2)\n",
        "    output_scaled_sx = W3 @ layer2_cas + b3\n",
        "    return output_scaled_sx\n",
        "\n",
        "# Create a CasADi function for the full ANN model (scaled_input -> scaled_output)\n",
        "ann_input_cas_sym = ca.SX.sym('ann_input_cas', input_dim_ann)\n",
        "ann_output_scaled_cas_sym = ann_casadi_model_scaled(ann_input_cas_sym, W1, b1, W2, b2, W3, b3)\n",
        "ann_model_casadi_scaled_func = ca.Function('ann_scaled', [ann_input_cas_sym], [ann_output_scaled_cas_sym])\n",
        "\n",
        "# Now create a wrapper that includes scaling and unscaling\n",
        "# For the NMPC, the model function f(xk, uk) should take unscaled xk, uk and return unscaled xk+1\n",
        "def ann_mpc_model_unscaled(xk_unscaled_sx, uk_unscaled_sx, input_scaler_obj, output_scaler_obj):\n",
        "    # xk_unscaled_sx (nx,1), uk_unscaled_sx (nu,1)\n",
        "    # Combine and scale input for ANN\n",
        "    ann_input_unscaled_sx = ca.vertcat(xk_unscaled_sx, uk_unscaled_sx)\n",
        "    \n",
        "    # Scaling: (val - min) / (max - min) = val * scale_ + offset_\n",
        "    # Need input_scaler.min_ and input_scaler.scale_ (which is 1/(max-min))\n",
        "    # We assume input_scaler was fit on [x_states, u_inputs]\n",
        "    # For CasADi, these scale/min values must be numerical constants\n",
        "    s_min = input_scaler.min_ \n",
        "    s_scale = input_scaler.scale_\n",
        "    \n",
        "    ann_input_scaled_sx = (ann_input_unscaled_sx - s_min.reshape(-1,1)) * s_scale.reshape(-1,1)\n",
        "    # If using CasADi's SX.sym for s_min, s_scale, they become part of the graph.\n",
        "    # For simplicity, let's assume they are fixed for the NMPC problem instance.\n",
        "\n",
        "    # Get scaled prediction from ANN\n",
        "    pred_output_scaled_sx = ann_model_casadi_scaled_func(ann_input_scaled_sx)\n",
        "    \n",
        "    # Unscale output: val_unscaled = val_scaled / scale_ + min_\n",
        "    o_min = output_scaler.min_\n",
        "    o_scale = output_scaler.scale_\n",
        "    \n",
        "    pred_output_unscaled_sx = pred_output_scaled_sx / o_scale.reshape(-1,1) + o_min.reshape(-1,1)\n",
        "    return pred_output_unscaled_sx # This is predicted x_k+1 (unscaled)\n",
        "\n",
        "print(\"CasADi wrapper for ANN model created.\")\n",
        "# Test the CasADi ANN function with an example input (requires numerical scalers)\n",
        "test_x_unscaled = np.array([0.5, 350.0])\n",
        "test_u_unscaled = np.array([300.0])\n",
        "test_ann_input_unscaled = np.concatenate((test_x_unscaled, test_u_unscaled))\n",
        "test_ann_input_scaled_np = input_scaler.transform(test_ann_input_unscaled.reshape(1,-1)).flatten()\n",
        "\n",
        "casadi_pred_scaled = ann_model_casadi_scaled_func(test_ann_input_scaled_np)\n",
        "casadi_pred_unscaled = output_scaler.inverse_transform(np.array(casadi_pred_scaled).reshape(1,-1))\n",
        "print(f\"Test CasADi ANN prediction (unscaled): {casadi_pred_unscaled}\")\n",
        "\n",
        "# For PyTorch model prediction for same input (for comparison)\n",
        "ann_model.eval()\n",
        "with torch.no_grad():\n",
        "    pytorch_pred_scaled = ann_model(torch.tensor(test_ann_input_scaled_np, dtype=torch.float32))\n",
        "    pytorch_pred_unscaled = output_scaler.inverse_transform(pytorch_pred_scaled.numpy().reshape(1,-1))\n",
        "print(f\"Test PyTorch ANN prediction (unscaled): {pytorch_pred_unscaled}\")\n",
        "assert np.allclose(casadi_pred_unscaled, pytorch_pred_unscaled, atol=1e-5), \"CasADi and PyTorch ANN outputs differ!\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6288ff",
      "metadata": {},
      "source": [
        "## 6. ANN-MPC Implementation and Simulation\n",
        "\n",
        "Now we set up the NMPC problem using the `ann_mpc_model_unscaled` as our dynamics function $f(x_k, u_k)$ within CasADi's integrator or direct transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fbd31a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANN-MPC Parameters\n",
        "Ts_ann_mpc = Ts_data_gen # Control interval same as ANN training data sampling\n",
        "Np_ann_mpc = 10      # Prediction horizon\n",
        "\n",
        "# Objective Function Weights (example)\n",
        "Q_Ca_ann = 100.0   # Weight for Ca tracking error\n",
        "Q_T_ann = 1.0     # Weight for T tracking error\n",
        "R_Tc_ann = 0.1    # Weight for Tc (coolant temp) magnitude\n",
        "S_Tc_ann = 0.5    # Weight for Tc rate of change\n",
        "\n",
        "# Constraints\n",
        "Tc_min_ann_mpc = 280.0; Tc_max_ann_mpc = 320.0\n",
        "delta_Tc_max_ann_mpc = 5.0 # K per Ts_ann_mpc\n",
        "Ca_min_ann_mpc = 0.05; Ca_max_ann_mpc = 0.8\n",
        "T_min_ann_mpc = 300.0; T_max_ann_mpc = 380.0\n",
        "\n",
        "# Setpoints for Ca and T\n",
        "Ca_sp_target = 0.3 # mol/m^3\n",
        "T_sp_target = 330  # K\n",
        "\n",
        "# --- CasADi ANN-MPC Setup ---\n",
        "opti_ann_mpc = ca.Opti()\n",
        "nx_ann = 2 # Ca, T\n",
        "nu_ann = 1 # Tc\n",
        "\n",
        "X_ann_pred_sym = opti_ann_mpc.variable(nx_ann, Np_ann_mpc + 1)\n",
        "U_ann_pred_sym = opti_ann_mpc.variable(nu_ann, Np_ann_mpc)\n",
        "\n",
        "x0_ann_param = opti_ann_mpc.parameter(nx_ann)\n",
        "u_prev_ann_param = opti_ann_mpc.parameter(nu_ann)\n",
        "Ca_sp_ann_param = opti_ann_mpc.parameter(Np_ann_mpc)\n",
        "T_sp_ann_param = opti_ann_mpc.parameter(Np_ann_mpc)\n",
        "\n",
        "obj_ann_mpc = 0\n",
        "for j in range(Np_ann_mpc):\n",
        "    obj_ann_mpc += Q_Ca_ann * (X_ann_pred_sym[0, j+1] - Ca_sp_ann_param[j])**2\n",
        "    obj_ann_mpc += Q_T_ann * (X_ann_pred_sym[1, j+1] - T_sp_ann_param[j])**2\n",
        "    obj_ann_mpc += R_Tc_ann * (U_ann_pred_sym[0, j])**2\n",
        "    delta_u_ann = U_ann_pred_sym[0, j] - (u_prev_ann_param[0] if j==0 else U_ann_pred_sym[0, j-1])\n",
        "    obj_ann_mpc += S_Tc_ann * delta_u_ann**2\n",
        "opti_ann_mpc.minimize(obj_ann_mpc)\n",
        "\n",
        "# Dynamic constraints using the ANN model\n",
        "opti_ann_mpc.subject_to(X_ann_pred_sym[:,0] == x0_ann_param)\n",
        "for j in range(Np_ann_mpc):\n",
        "    # Current state x_j and input u_j for ANN model\n",
        "    xk_unscaled_for_ann = X_ann_pred_sym[:,j]\n",
        "    uk_unscaled_for_ann = U_ann_pred_sym[:,j]\n",
        "    \n",
        "    # Call the CasADi wrapper for the ANN model\n",
        "    # Need to pass the actual scaler objects, or hardcode their params in a pure CasADi func\n",
        "    # For this to work within opti, ann_mpc_model_unscaled needs to be a pure CasADi function\n",
        "    # Let's define a new pure CasADi version using the numerical scaler values\n",
        "    s_min_vals = input_scaler.min_\n",
        "    s_scale_vals = input_scaler.scale_\n",
        "    o_min_vals = output_scaler.min_\n",
        "    o_scale_vals = output_scaler.scale_\n",
        "    \n",
        "    def ann_pure_casadi_f(xk_sx, uk_sx):\n",
        "        ann_input_unsc_sx = ca.vertcat(xk_sx, uk_sx)\n",
        "        ann_input_sc_sx = (ann_input_unsc_sx - s_min_vals.reshape(-1,1)) * s_scale_vals.reshape(-1,1)\n",
        "        pred_out_sc_sx = ann_model_casadi_scaled_func(ann_input_sc_sx)\n",
        "        pred_out_unsc_sx = pred_out_sc_sx / o_scale_vals.reshape(-1,1) + o_min_vals.reshape(-1,1)\n",
        "        return pred_out_unsc_sx\n",
        "    \n",
        "    x_next_pred_ann = ann_pure_casadi_f(xk_unscaled_for_ann, uk_unscaled_for_ann)\n",
        "    opti_ann_mpc.subject_to(X_ann_pred_sym[:,j+1] == x_next_pred_ann)\n",
        "    \n",
        "    # Other constraints\n",
        "    opti_ann_mpc.subject_to(opti_ann_mpc.bounded(Tc_min_ann_mpc, U_ann_pred_sym[0,j], Tc_max_ann_mpc))\n",
        "    delta_u_c_ann = U_ann_pred_sym[0,j] - (u_prev_ann_param[0] if j==0 else U_ann_pred_sym[0,j-1])\n",
        "    opti_ann_mpc.subject_to(opti_ann_mpc.bounded(-delta_Tc_max_ann_mpc, delta_u_c_ann, delta_Tc_max_ann_mpc))\n",
        "    opti_ann_mpc.subject_to(opti_ann_mpc.bounded(Ca_min_ann_mpc, X_ann_pred_sym[0,j+1], Ca_max_ann_mpc))\n",
        "    opti_ann_mpc.subject_to(opti_ann_mpc.bounded(T_min_ann_mpc, X_ann_pred_sym[1,j+1], T_max_ann_mpc))\n",
        "\n",
        "opts_ann_mpc = {'ipopt.max_iter': 100, 'ipopt.print_level': 0, 'print_time': 0,\n",
        "                'ipopt.acceptable_tol': 1e-6}\n",
        "opti_ann_mpc.solver('ipopt', opts_ann_mpc)\n",
        "print(\"ANN-MPC problem formulated.\")\n",
        "\n",
        "# --- ANN-MPC Simulation Loop ---\n",
        "sim_time_ann_mpc_total = 60 # min\n",
        "num_sim_steps_ann_mpc = int(sim_time_ann_mpc_total / Ts_ann_mpc)\n",
        "\n",
        "x_plant_ann_mpc_current = np.array([0.6, 340.0]) # True initial plant state [Ca, T]\n",
        "u_plant_ann_mpc_prev = np.array([300.0])      # Initial previous input Tc\n",
        "\n",
        "Ca_sp_horizon_ann = np.full(Np_ann_mpc, Ca_sp_target)\n",
        "T_sp_horizon_ann = np.full(Np_ann_mpc, T_sp_target)\n",
        "\n",
        "t_log_ann_mpc = np.zeros(num_sim_steps_ann_mpc + 1)\n",
        "X_log_ann_mpc_plant = np.zeros((nx_ann, num_sim_steps_ann_mpc + 1))\n",
        "U_log_ann_mpc = np.zeros((nu_ann, num_sim_steps_ann_mpc))\n",
        "\n",
        "X_log_ann_mpc_plant[:, 0] = x_plant_ann_mpc_current\n",
        "t_log_ann_mpc[0] = 0\n",
        "\n",
        "U_guess_ann_mpc = np.full((nu_ann, Np_ann_mpc), np.mean([Tc_min_ann_mpc, Tc_max_ann_mpc]))\n",
        "X_guess_ann_mpc = np.tile(x_plant_ann_mpc_current.reshape(nx_ann,1), (1, Np_ann_mpc + 1))\n",
        "\n",
        "print(f\"Starting ANN-MPC simulation for {num_sim_steps_ann_mpc} steps...\")\n",
        "for k in range(num_sim_steps_ann_mpc):\n",
        "    current_t_ann_mpc = k * Ts_ann_mpc\n",
        "    print(f\"ANN-MPC Step {k+1}/{num_sim_steps_ann_mpc} (t={current_t_ann_mpc:.1f} min)\", end='\\r')\n",
        "    \n",
        "    opti_ann_mpc.set_value(x0_ann_param, x_plant_ann_mpc_current)\n",
        "    opti_ann_mpc.set_value(u_prev_ann_param, u_plant_ann_mpc_prev)\n",
        "    opti_ann_mpc.set_value(Ca_sp_ann_param, Ca_sp_horizon_ann)\n",
        "    opti_ann_mpc.set_value(T_sp_ann_param, T_sp_horizon_ann) \n",
        "    \n",
        "    opti_ann_mpc.set_initial(X_ann_pred_sym, X_guess_ann_mpc)\n",
        "    opti_ann_mpc.set_initial(U_ann_pred_sym, U_guess_ann_mpc)\n",
        "    \n",
        "    try:\n",
        "        sol_ann_mpc = opti_ann_mpc.solve()\n",
        "        U_opt_ann_mpc = sol_ann_mpc.value(U_ann_pred_sym)\n",
        "        X_pred_ann_mpc = sol_ann_mpc.value(X_ann_pred_sym)\n",
        "        u_to_apply_ann_mpc = U_opt_ann_mpc[:, 0]\n",
        "        X_guess_ann_mpc = np.hstack((X_pred_ann_mpc[:, 1:], X_pred_ann_mpc[:, -1].reshape(nx_ann,1)))\n",
        "        U_guess_ann_mpc = np.hstack((U_opt_ann_mpc[:, 1:], U_opt_ann_mpc[:, -1].reshape(nu_ann,1)))\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\nANN-MPC Solver failed at step {k+1}: {e}. Using previous input.\")\n",
        "        u_to_apply_ann_mpc = u_plant_ann_mpc_prev.flatten()\n",
        "        U_guess_ann_mpc = np.full((nu_ann, Np_ann_mpc), u_plant_ann_mpc_prev[0])\n",
        "        X_guess_ann_mpc = np.tile(x_plant_ann_mpc_current.reshape(nx_ann,1), (1, Np_ann_mpc + 1))\n",
        "\n",
        "    U_log_ann_mpc[:, k] = u_to_apply_ann_mpc\n",
        "    \n",
        "    # True Plant Evolution (using CSTR ODEs)\n",
        "    plant_sol_ann_mpc_step = solve_ivp(cstr_ode_numpy, \n",
        "                                     [current_t_ann_mpc, current_t_ann_mpc + Ts_ann_mpc], \n",
        "                                     x_plant_ann_mpc_current, \n",
        "                                     args=(u_to_apply_ann_mpc[0], cstr_params_list), \n",
        "                                     dense_output=False, t_eval=[current_t_ann_mpc + Ts_ann_mpc],\n",
        "                                     method='RK45')\n",
        "    x_plant_ann_mpc_current = plant_sol_ann_mpc_step.y[:,-1]\n",
        "    \n",
        "    X_log_ann_mpc_plant[:, k+1] = x_plant_ann_mpc_current\n",
        "    t_log_ann_mpc[k+1] = current_t_ann_mpc + Ts_ann_mpc\n",
        "    u_plant_ann_mpc_prev = u_to_apply_ann_mpc.reshape(nu_ann,1)\n",
        "\n",
        "print(\"\\nANN-MPC simulation finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7693e1c1",
      "metadata": {},
      "source": [
        "## 7. Visualizing ANN-MPC Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d2e12ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_ann, axs_ann = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
        "fig_ann.suptitle(f'ANN-MPC for CSTR Control', fontsize=16)\n",
        "time_plot_ann = t_log_ann_mpc\n",
        "time_u_plot_ann = t_log_ann_mpc[:-1]\n",
        "\n",
        "# CA\n",
        "axs_ann[0].plot(time_plot_ann, X_log_ann_mpc_plant[0, :], 'b-', label='$C_{A,plant}$')\n",
        "axs_ann[0].axhline(Ca_sp_target, color='r', linestyle=':', label='$C_{A,sp}$')\n",
        "axs_ann[0].axhline(Ca_min_ann_mpc, color='m', ls='--', label='$C_{A,min}$ Cons.')\n",
        "axs_ann[0].axhline(Ca_max_ann_mpc, color='m', ls='--', label='$C_{A,max}$ Cons.')\n",
        "axs_ann[0].set_ylabel('$C_A$ (mol/m^3)'); axs_ann[0].grid(True); axs_ann[0].legend()\n",
        "\n",
        "# T\n",
        "axs_ann[1].plot(time_plot_ann, X_log_ann_mpc_plant[1, :], 'b-', label='$T_{plant}$ (K)')\n",
        "axs_ann[1].axhline(T_sp_target, color='r', linestyle=':', label='$T_{sp}$')\n",
        "axs_ann[1].axhline(T_min_ann_mpc, color='m', ls='--', label='$T_{min}$ Cons.')\n",
        "axs_ann[1].axhline(T_max_ann_mpc, color='m', ls='--', label='$T_{max}$ Cons.')\n",
        "axs_ann[1].set_ylabel('Temperature T (K)'); axs_ann[1].grid(True); axs_ann[1].legend()\n",
        "\n",
        "# Control Input Tc\n",
        "axs_ann[2].step(time_u_plot_ann, U_log_ann_mpc[0, :], 'k-', where='post', label='$T_c$ (K)')\n",
        "axs_ann[2].axhline(Tc_max_ann_mpc, color='m', ls='--', label='$T_{c,max}$ Cons.')\n",
        "axs_ann[2].axhline(Tc_min_ann_mpc, color='m', ls='--', label='$T_{c,min}$ Cons.')\n",
        "axs_ann[2].set_ylabel('Coolant Temp $T_c$ (K)'); axs_ann[2].set_xlabel('Time (min)'); axs_ann[2].grid(True); axs_ann[2].legend()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80510e3",
      "metadata": {},
      "source": [
        "## 8. Discussion and Exercises\n",
        "\n",
        "*   **ANN Model Accuracy:** How well did the ANN model (from validation in Section 4) predict the true CSTR behavior? If the ANN model is inaccurate, the ANN-MPC performance will suffer.\n",
        "*   **ANN-MPC Performance:** Does the ANN-MPC effectively track the setpoints for $C_A$ and $T$? How does it handle constraints?\n",
        "*   **Comparison:**\n",
        "    *   If you implemented NMPC with the true CSTR model (e.g., in Notebook 3.3 or 8.6 from main course), how does this ANN-MPC compare?\n",
        "    *   How might an LMPC (linearized CSTR model) perform on this task, especially if the setpoint change is large?\n",
        "*   **Tuning:** Experiment with the ANN-MPC weights ($Q_{Ca}, Q_T, R_{Tc}, S_{Tc}$) and the prediction horizon $N_p$. How do they affect closed-loop behavior and computational demand?\n",
        "*   **Extrapolation:** The ANN model is only reliable within the range of data it was trained on. If the MPC tries to drive the system far outside this range, the ANN's predictions might become poor, leading to bad control actions. How could this be mitigated conceptually?\n",
        "*   **Computational Cost:** Is the ANN-MPC significantly slower or faster to solve per step compared to an NMPC using the analytical ODEs (if you have a comparison)? This depends on the ANN complexity and the ODE complexity.\n",
        "*   **Alternative ANN Structures:** We used a simple FNN. Would an RNN (like LSTM or GRU) potentially offer better performance for modeling dynamics, especially if there are longer-term dependencies not captured by a simple FNN taking only $x_k, u_k$ as input?\n",
        "\n",
        "This notebook demonstrates that a learned ANN model can indeed be used as the predictive core for an NMPC controller. The success heavily relies on the quality of the training data and the generalization capability of the ANN."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e45a11",
      "metadata": {},
      "source": [
        "## 9. Key Takeaways\n",
        "\n",
        "*   Artificial Neural Networks can be trained to learn the dynamics of nonlinear systems from input-output data.\n",
        "*   A trained ANN can serve as the prediction model within an NMPC framework.\n",
        "*   Integrating ANNs (especially from frameworks like PyTorch/TensorFlow) into CasADi for NMPC often involves re-implementing the ANN structure symbolically or using ONNX conversion to leverage CasADi's automatic differentiation for the NLP solver.\n",
        "*   The performance of ANN-MPC is highly dependent on the quality of the ANN model, which in turn depends on the training data, network architecture, and training process.\n",
        "*   Challenges include the \"black-box\" nature, potential for poor extrapolation, and ensuring the ANN model is suitable for multi-step prediction within the MPC horizon.\n",
        "\n",
        "In the next notebook (**Notebook 6.3: MPC with Physics-Informed Neural Network (PINN) Models**), we will explore PINNs, a hybrid approach that attempts to combine the learning power of ANNs with known physical laws to create more robust and data-efficient models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.x"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
