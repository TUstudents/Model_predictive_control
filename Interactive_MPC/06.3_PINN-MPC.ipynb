{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3b6c3876",
      "metadata": {},
      "source": [
        "# Notebook 6.3: MPC with Physics-Informed Neural Network (PINN) Models (PINN-MPC)\n",
        "\n",
        "In Notebook 6.2, we explored using standard Artificial Neural Networks (ANNs) for modeling system dynamics within an MPC framework. While ANNs are powerful function approximators, they learn solely from data and have no inherent understanding of the underlying physical laws governing the system. This can lead to physically implausible predictions, poor extrapolation, and a need for large datasets.\n",
        "\n",
        "**Physics-Informed Neural Networks (PINNs)** offer a compelling hybrid approach. They integrate known physical laws (typically expressed as Ordinary or Partial Differential Equations - ODEs/PDEs) directly into the neural network's training process. The network is trained to satisfy both the observed data *and* the governing equations.\n",
        "\n",
        "**Goals of this Notebook:**\n",
        "1.  Understand the core concept of PINNs.\n",
        "2.  Select a simple dynamic system with known ODEs.\n",
        "3.  Formulate the composite PINN loss function ($L_{data} + w_{physics}L_{physics} + w_{BC/IC}L_{BC/IC}$).\n",
        "4.  Implement and train a PINN using **PyTorch** to learn the system's solution trajectory (and potentially unknown parameters within the ODEs).\n",
        "5.  Validate the trained PINN model's adherence to physics and data.\n",
        "6.  Discuss how a trained PINN, representing system dynamics, can be integrated into an NMPC framework (conceptually or with a simplified example).\n",
        "7.  Highlight the advantages (e.g., data efficiency, physical consistency) and challenges of PINN-MPC."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb0c58f",
      "metadata": {},
      "source": [
        "## 1. Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02e7eb4a",
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "libcudnn.so.9: cannot open shared object file: No such file or directory",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Model_predictive_control/.venv/lib/python3.12/site-packages/torch/__init__.py:405\u001b[39m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    404\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[31mImportError\u001b[39m: libcudnn.so.9: cannot open shared object file: No such file or directory"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.integrate import solve_ivp # For generating reference solutions\n",
        "import casadi as ca # For conceptual MPC integration\n",
        "\n",
        "# Optional: for nicer plots\n",
        "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 6)})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c6b4855",
      "metadata": {},
      "source": [
        "## 2. The Dynamic System: A Damped Oscillator\n",
        "\n",
        "Let's choose a simple, well-understood system: a damped linear oscillator. Its dynamics are described by a second-order ODE:\n",
        "$$ m \\frac{d^2x}{dt^2} + c \\frac{dx}{dt} + kx = F(t) $$\n",
        "Where $x$ is position, $m$ is mass, $c$ is damping coefficient, $k$ is spring constant, and $F(t)$ is an external force.\n",
        "\n",
        "We can rewrite this as a system of two first-order ODEs. Let $x_1 = x$ and $x_2 = \\frac{dx}{dt}$ (velocity).\n",
        "$$ \\frac{dx_1}{dt} = x_2 $$\n",
        "$$ \\frac{dx_2}{dt} = \\frac{1}{m} (F(t) - c x_2 - k x_1) $$\n",
        "\n",
        "For this PINN example, our neural network $\\mathcal{N}(t; \\theta)$ will aim to approximate $x_1(t)$ (and implicitly $x_2(t)$ via its derivative). We might also try to learn one of the parameters, say $c$ or $k$, if it were unknown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4498713d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# System Parameters (True values for generating data/reference)\n",
        "m_true_osc = 1.0\n",
        "k_true_osc = 4.0 * (np.pi**2) # k for a natural frequency of 2 Hz (period 0.5s) if c=0\n",
        "c_true_osc = 0.5 # Damping coefficient\n",
        "F_ext_func = lambda t: 0.0 # No external force for this example (free oscillation)\n",
        "\n",
        "# ODE function for scipy solver (reference solution)\n",
        "def damped_oscillator_ode(t, y_state, m, k, c, F_func):\n",
        "    x1, x2 = y_state # x1 = position, x2 = velocity\n",
        "    dx1_dt = x2\n",
        "    dx2_dt = (F_func(t) - c * x2 - k * x1) / m\n",
        "    return [dx1_dt, dx2_dt]\n",
        "\n",
        "# Time domain for simulation and PINN training\n",
        "t_start_osc = 0.0\n",
        "t_end_osc = 2.0\n",
        "num_t_points_ref = 200\n",
        "t_eval_ref = np.linspace(t_start_osc, t_end_osc, num_t_points_ref)\n",
        "\n",
        "# Initial conditions\n",
        "x1_0 = 1.0 # Initial position\n",
        "x2_0 = 0.0 # Initial velocity\n",
        "y0_osc = [x1_0, x2_0]\n",
        "\n",
        "# Generate reference solution using scipy ODE solver\n",
        "sol_ref = solve_ivp(damped_oscillator_ode, [t_start_osc, t_end_osc], y0_osc, \n",
        "                    args=(m_true_osc, k_true_osc, c_true_osc, F_ext_func),\n",
        "                    dense_output=True, t_eval=t_eval_ref)\n",
        "x1_ref = sol_ref.y[0,:]\n",
        "x2_ref = sol_ref.y[1,:]\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(t_eval_ref, x1_ref, label='True $x_1(t)$ (Position)')\n",
        "plt.plot(t_eval_ref, x2_ref, label='True $x_2(t)$ (Velocity)')\n",
        "plt.xlabel('Time (s)'); plt.ylabel('State Value'); plt.title('Reference Damped Oscillator Solution')\n",
        "plt.legend(); plt.grid(True); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c63d0a1",
      "metadata": {},
      "source": [
        "## 3. PINN Formulation\n",
        "\n",
        "**Neural Network $\\mathcal{N}(t; \\theta)$:** This NN will take time $t$ as input and output the predicted position $\\hat{x}_1(t)$. The velocity $\\hat{x}_2(t)$ can be obtained by $\\frac{d\\hat{x}_1}{dt}$ using automatic differentiation.\n",
        "\n",
        "**Loss Function Components:**\n",
        "1.  **$L_{data}$ (Data Mismatch Loss):** If we have some (noisy) measurements of $x_1(t)$ at specific time points $t_i^{data}$.\n",
        "    $L_{data} = \\frac{1}{N_{data}} \\sum_{i=1}^{N_{data}} (\\mathcal{N}(t_i^{data}; \\theta) - x_{1,data}^{(i)})^2$\n",
        "2.  **$L_{physics}$ (ODE Residual Loss):** Enforces the governing ODEs. The residual for the second ODE is:\n",
        "    $r_{ODE}(t) = m \\frac{d^2\\mathcal{N}}{dt^2} + c \\frac{d\\mathcal{N}}{dt} + k \\mathcal{N} - F(t)$\n",
        "    $L_{physics} = \\frac{1}{N_{colloc}} \\sum_{j=1}^{N_{colloc}} (r_{ODE}(t_j^{colloc}))^2$\n",
        "    This is evaluated at a set of collocation points $t_j^{colloc}$ distributed across the time domain. The derivatives $\\frac{d\\mathcal{N}}{dt}$ and $\\frac{d^2\\mathcal{N}}{dt^2}$ are computed using PyTorch's autograd.\n",
        "3.  **$L_{IC}$ (Initial Condition Loss):** Enforces the known initial conditions.\n",
        "    $L_{IC} = (\\mathcal{N}(t_0; \\theta) - x_{1,0})^2 + (\\frac{d\\mathcal{N}}{dt}|_{t_0} - x_{2,0})^2$\n",
        "\n",
        "**Total Loss:** $L = w_{data} L_{data} + w_{physics} L_{physics} + w_{IC} L_{IC}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51cf4139",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PINN Neural Network Definition\n",
        "class OscillatorPINN(nn.Module):\n",
        "    def __init__(self, hidden_sizes=[32, 32]):\n",
        "        super(OscillatorPINN, self).__init__()\n",
        "        layers = [nn.Linear(1, hidden_sizes[0]), nn.Tanh()] # Input is time (1D)\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], 1)) # Output is x1 (position, 1D)\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # For learning parameters (optional)\n",
        "        # self.log_c = nn.Parameter(torch.tensor([np.log(c_true_osc*0.8)], dtype=torch.float32)) # Learn c\n",
        "        # self.log_k = nn.Parameter(torch.tensor([np.log(k_true_osc*1.2)], dtype=torch.float32)) # Learn k\n",
        "\n",
        "    def forward(self, t):\n",
        "        # t should be a column vector with requires_grad=True for derivative calculations\n",
        "        return self.net(t)\n",
        "\n",
        "# Instantiate PINN\n",
        "pinn_model = OscillatorPINN(hidden_sizes=[64, 64, 64])\n",
        "\n",
        "# Define system parameters (can be made learnable if desired)\n",
        "m_pinn = torch.tensor([m_true_osc], dtype=torch.float32, requires_grad=False)\n",
        "k_pinn = torch.tensor([k_true_osc], dtype=torch.float32, requires_grad=False) \n",
        "c_pinn = torch.tensor([c_true_osc], dtype=torch.float32, requires_grad=False)\n",
        "# To learn c and k, use pinn_model.log_c and pinn_model.log_k, then c = torch.exp(self.log_c), etc.\n",
        "\n",
        "def compute_oscillator_ode_residual(pinn, t_batch, m, k, c, F_func):\n",
        "    t_batch.requires_grad_(True)\n",
        "    x1_pred = pinn(t_batch)\n",
        "    \n",
        "    # First derivative dx1/dt (velocity x2)\n",
        "    dx1_dt = torch.autograd.grad(x1_pred, t_batch, \n",
        "                                  grad_outputs=torch.ones_like(x1_pred), \n",
        "                                  create_graph=True)[0]\n",
        "    # Second derivative d^2x1/dt^2 (acceleration)\n",
        "    d2x1_dt2 = torch.autograd.grad(dx1_dt, t_batch, \n",
        "                                   grad_outputs=torch.ones_like(dx1_dt), \n",
        "                                   create_graph=True)[0]\n",
        "    \n",
        "    F_t_batch = torch.tensor([F_func(ti.item()) for ti in t_batch], dtype=torch.float32).reshape_as(x1_pred)\n",
        "    \n",
        "    # ODE residual: m*x1_tt + c*x1_t + k*x1 - F = 0\n",
        "    residual = m * d2x1_dt2 + c * dx1_dt + k * x1_pred - F_t_batch\n",
        "    return residual, x1_pred, dx1_dt # also return x1, x2 for IC loss\n",
        "\n",
        "print(\"PINN model and residual function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc77920",
      "metadata": {},
      "source": [
        "## 4. Training the PINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fada95e",
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer_pinn = optim.Adam(pinn_model.parameters(), lr=1e-3)\n",
        "scheduler_pinn = optim.lr_scheduler.StepLR(optimizer_pinn, step_size=500, gamma=0.7)\n",
        "\n",
        "# Training Data Points (sparse subset of reference for L_data)\n",
        "num_data_pts_pinn = 20\n",
        "idx_data = np.random.choice(num_t_points_ref, num_data_pts_pinn, replace=False)\n",
        "t_data_pinn = torch.tensor(t_eval_ref[idx_data], dtype=torch.float32).reshape(-1,1)\n",
        "x1_data_pinn = torch.tensor(x1_ref[idx_data], dtype=torch.float32).reshape(-1,1)\n",
        "\n",
        "# Collocation Points for L_physics (more dense than data points)\n",
        "num_colloc_pts_pinn = 200\n",
        "t_colloc_pinn = torch.linspace(t_start_osc, t_end_osc, num_colloc_pts_pinn, \n",
        "                               dtype=torch.float32).reshape(-1,1)\n",
        "\n",
        "# Initial condition time point\n",
        "t_ic_pinn = torch.tensor([[t_start_osc]], dtype=torch.float32)\n",
        "\n",
        "# Loss weights\n",
        "w_data = 100.0\n",
        "w_physics = 1.0\n",
        "w_ic = 100.0\n",
        "\n",
        "epochs_pinn = 2000 # More epochs often needed for PINNs\n",
        "losses_pinn = {'total': [], 'data': [], 'physics': [], 'ic': []}\n",
        "\n",
        "print(\"Starting PINN training...\")\n",
        "for epoch in range(epochs_pinn):\n",
        "    pinn_model.train()\n",
        "    optimizer_pinn.zero_grad()\n",
        "    \n",
        "    # Data loss\n",
        "    x1_pred_data = pinn_model(t_data_pinn)\n",
        "    loss_data = torch.mean((x1_pred_data - x1_data_pinn)**2)\n",
        "    \n",
        "    # Physics loss (ODE residual)\n",
        "    ode_res, _, _ = compute_oscillator_ode_residual(pinn_model, t_colloc_pinn, m_pinn, k_pinn, c_pinn, F_ext_func)\n",
        "    loss_physics = torch.mean(ode_res**2)\n",
        "    \n",
        "    # Initial condition loss\n",
        "    _, x1_pred_ic, x2_pred_ic = compute_oscillator_ode_residual(pinn_model, t_ic_pinn, m_pinn, k_pinn, c_pinn, F_ext_func)\n",
        "    loss_ic_x1 = (x1_pred_ic - x1_0)**2\n",
        "    loss_ic_x2 = (x2_pred_ic - x2_0)**2\n",
        "    loss_ic = torch.mean(loss_ic_x1 + loss_ic_x2)\n",
        "    \n",
        "    total_loss = w_data * loss_data + w_physics * loss_physics + w_ic * loss_ic\n",
        "    \n",
        "    total_loss.backward()\n",
        "    optimizer_pinn.step()\n",
        "    scheduler_pinn.step()\n",
        "    \n",
        "    losses_pinn['total'].append(total_loss.item())\n",
        "    losses_pinn['data'].append(loss_data.item())\n",
        "    losses_pinn['physics'].append(loss_physics.item())\n",
        "    losses_pinn['ic'].append(loss_ic.item())\n",
        "    \n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs_pinn}], Total Loss: {total_loss.item():.4e}, \"\n",
        "              f\"Data: {loss_data.item():.2e}, Physics: {loss_physics.item():.2e}, IC: {loss_ic.item():.2e}\")\n",
        "\n",
        "print(\"PINN training complete.\")\n",
        "\n",
        "# Plot PINN training losses\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(losses_pinn['total'], label='Total Loss')\n",
        "plt.plot(losses_pinn['data'], label=f'Data Loss (w={w_data})', alpha=0.7)\n",
        "plt.plot(losses_pinn['physics'], label=f'Physics Loss (w={w_physics})', alpha=0.7)\n",
        "plt.plot(losses_pinn['ic'], label=f'IC Loss (w={w_ic})', alpha=0.7)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log')\n",
        "plt.title('PINN Training Progress'); plt.legend(); plt.grid(True); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ffeedb",
      "metadata": {},
      "source": [
        "## 5. Validating the Trained PINN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae25d76d",
      "metadata": {},
      "outputs": [],
      "source": [
        "pinn_model.eval()\n",
        "t_dense_test = torch.linspace(t_start_osc, t_end_osc, 200, dtype=torch.float32).reshape(-1,1)\n",
        "with torch.no_grad():\n",
        "    x1_pinn_pred_test = pinn_model(t_dense_test).numpy()\n",
        "\n",
        "# For x2, we need gradients, so re-enable grad for this part\n",
        "t_dense_test_grad = t_dense_test.clone().requires_grad_(True)\n",
        "x1_pinn_grad_intermediate = pinn_model(t_dense_test_grad)\n",
        "x2_pinn_pred_test = torch.autograd.grad(x1_pinn_grad_intermediate, t_dense_test_grad, \n",
        "                                         grad_outputs=torch.ones_like(x1_pinn_grad_intermediate), \n",
        "                                         create_graph=False)[0].detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(t_eval_ref, x1_ref, 'b-', label='True $x_1$ (Position)')\n",
        "plt.plot(t_dense_test.numpy(), x1_pinn_pred_test, 'r--', label='PINN Predicted $x_1$')\n",
        "plt.scatter(t_data_pinn.numpy(), x1_data_pinn.numpy(), color='k', s=30, label='Training Data Pts ($x_1$)')\n",
        "plt.xlabel('Time (s)'); plt.ylabel('$x_1$ (Position)'); plt.title('PINN vs. True Solution ($x_1$)')\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(t_eval_ref, x2_ref, 'b-', label='True $x_2$ (Velocity)')\n",
        "plt.plot(t_dense_test.numpy(), x2_pinn_pred_test, 'r--', label='PINN Predicted $x_2$ (from $dx_1/dt$)')\n",
        "plt.xlabel('Time (s)'); plt.ylabel('$x_2$ (Velocity)'); plt.title('PINN vs. True Solution ($x_2$)')\n",
        "plt.legend(); plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check ODE residual with trained PINN\n",
        "t_colloc_check = torch.linspace(t_start_osc, t_end_osc, 100, dtype=torch.float32).reshape(-1,1)\n",
        "ode_res_check, _, _ = compute_oscillator_ode_residual(pinn_model, t_colloc_check, m_pinn, k_pinn, c_pinn, F_ext_func)\n",
        "print(f\"Mean squared ODE residual on test collocation points: {torch.mean(ode_res_check**2).item():.4e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a0a106",
      "metadata": {},
      "source": [
        "## 6. Conceptual Integration of PINN into MPC\n",
        "\n",
        "Once a PINN is trained to represent the system dynamics $x_{k+1} = f_{PINN}(x_k, u_k, \\Delta t)$ or $\\dot{x} = f_{PINN}(x,u)$, it can, in principle, replace the model in an NMPC formulation.\n",
        "\n",
        "**If PINN learns $x(t)$ directly (like $\\mathcal{N}(t)$ in our example):**\n",
        "The PINN itself gives $\\hat{x}_1(t_0+j \\cdot T_{s,mpc})$. Its derivatives give other states. This can be used for prediction. For control inputs $u_k$ that are piecewise constant, the ODE for PINN training would be $m \\ddot{x}_1 + c \\dot{x}_1 + k x_1 = u_k$. The PINN $\\mathcal{N}(t, u_k; \\theta)$ would predict $x_1(t)$ for a given $u_k$ over an interval.\n",
        "\n",
        "**If PINN learns $f_{discrete}(x_k, u_k)$ or $f_{continuous}(x,u)$:**\n",
        "This is more direct for MPC. The PINN acts as the $f(\\cdot)$ in $x_{k+1}=f(x_k, u_k)$.\n",
        "   1.  The input to this PINN would be $(x_k, u_k)$.\n",
        "   2.  The output would be $\\Delta x_k$ or $x_{k+1}$.\n",
        "   3.  The physics loss would enforce the difference/differential equation residual.\n",
        "\n",
        "**Integration with CasADi for PINN-MPC:**\n",
        "Similar to ANN-MPC (Notebook 6.2), the trained PyTorch PINN model needs to be callable by CasADi, and CasADi needs its gradients.\n",
        "1.  **Re-implement PINN in CasADi:** If the PINN structure is a standard FNN, its forward pass (and the computation of derivatives for the physics residual if it was part of the output) can be re-implemented using CasADi symbolic variables with the trained PyTorch weights. This gives CasADi full symbolic access.\n",
        "2.  **ONNX Export/Import:** Export the PyTorch PINN to ONNX, then import into CasADi. This is suitable for more complex network architectures.\n",
        "3.  **`ca.Function.external`:** For black-box calls, but getting correct gradients for the NLP solver through the external function (which itself uses PyTorch's autograd) can be tricky without specialized interfaces.\n",
        "\n",
        "**Example NMPC Objective (if PINN provides $x_{k+1} = f_{PINN}(x_k, u_k)$ for the oscillator):**\n",
        "Minimize $J = \\sum (x_{1,pred} - x_{1,sp})^2 + R \\cdot u^2$\n",
        "Subject to dynamics: $x_{k+1,pred} = f_{PINN}(x_{k,pred}, u_k)$ and input constraints.\n",
        "\n",
        "**Challenges:**\n",
        "-   **Computational Cost:** Evaluating a PINN (and its derivatives for the physics part, if the PINN itself is the ODE solver) inside an NMPC loop can be expensive.\n",
        "-   **Gradient Propagation:** Ensuring correct and efficient gradient propagation from the MPC cost/constraints back through the PINN model to the control inputs $\\mathbf{U}_k$ is key.\n",
        "-   **Complexity of PINN Training:** Training PINNs well, especially for systems with inputs $u_k$ that change over the MPC horizon, is an active research area and more involved than standard supervised ANN training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98148ad9",
      "metadata": {},
      "source": [
        "## 7. Advantages and Discussion\n",
        "\n",
        "*   **Data Efficiency:** PINNs can often learn from sparser data than pure ANNs because the physics provides strong regularization.\n",
        "*   **Physical Consistency:** Predictions are more likely to be physically plausible and generalize better, especially for extrapolation where data is scarce but physics holds.\n",
        "*   **Parameter Identification:** PINNs can simultaneously learn the system's solution and unknown physical parameters within the ODEs/PDEs.\n",
        "*   **Handling Systems with Known Structure but Unknown Parameters:** Ideal for many engineering systems, including potentially parts of bioreactor models where overall mass balances are known but kinetic parameters are uncertain or need refinement.\n",
        "\n",
        "PINN-MPC is a rapidly evolving field. While the implementation can be more complex than standard ANN-MPC, the potential benefits in terms of model robustness and data efficiency are significant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adc00df",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "*   PINNs combine data-driven learning with known physical laws (ODEs/PDEs) by including physics-based residuals in the loss function.\n",
        "*   Automatic differentiation is crucial for computing derivatives needed for the physics residuals and for training.\n",
        "*   PINNs can lead to more data-efficient, physically consistent, and generalizable models compared to purely data-driven ANNs.\n",
        "*   Integrating a trained PINN into an NMPC framework requires careful consideration of how the PINN represents the system dynamics and how its gradients are provided to the NLP solver.\n",
        "*   This approach is promising for systems where some physical knowledge exists but a full first-principles model is difficult to obtain or has significant uncertainties.\n",
        "\n",
        "In the next notebook (**Notebook 6.4: MPC with Gaussian Process (GP) Models**), we will explore Gaussian Processes, another powerful data-driven modeling technique that excels at uncertainty quantification."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
