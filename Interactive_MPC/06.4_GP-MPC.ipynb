{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 6.4: MPC with Gaussian Process (GP) Models (GP-MPC) using GPyTorch\n",
        "\n",
        "This notebook explores using **Gaussian Processes (GPs)** for data-driven MPC. We will use **GPyTorch**, a modern, flexible library for building GP models in PyTorch.\n",
        "\n",
        "GPs are a Bayesian, non-parametric approach that provide both a mean prediction and, crucially, a measure of **uncertainty** (variance). We will train a GPyTorch model to learn system dynamics and discuss how it fits into the MPC framework, highlighting the utility of uncertainty quantification.\n",
        "\n",
        "**Goals of this Notebook:**\n",
        "1.  Introduce Gaussian Processes using the PyTorch-native `GPyTorch` library.\n",
        "2.  Train a GPyTorch model to learn the one-step-ahead dynamics of a nonlinear system.\n",
        "3.  Visualize the GPyTorch model's mean prediction and confidence intervals.\n",
        "4.  Discuss the challenge of integrating PyTorch-based models (GPyTorch) with CasADi-based NMPC.\n",
        "5.  Simulate a conceptual NMPC loop (using the analytical model for internal MPC predictions) to illustrate the control framework.\n",
        "6.  Discuss how GP uncertainty can inform robust MPC and how libraries like `BoTorch` leverage this for Bayesian Optimization and Active Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importing Libraries\n",
        "\n",
        "**Installation (if you haven't already):**\n",
        "Make sure your virtual environment from Notebook 0.0 is activated.\n",
        "```bash\n",
        "uv pip install gpytorch botorch # botorch for Bayesian Opt / Active Learning (uses gpytorch)\n",
        "```\n",
        "*(Note: We use BoTorch conceptually here, not in a full active-learning MPC example, which is a highly advanced topic)*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "libcudnn.so.9: cannot open shared object file: No such file or directory",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m solve_ivp\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcasadi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mca\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpytorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbotorch\u001b[39;00m \u001b[38;5;66;03m# Mentioned for context\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Model_predictive_control/.venv/lib/python3.12/site-packages/torch/__init__.py:405\u001b[39m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    404\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[31mImportError\u001b[39m: libcudnn.so.9: cannot open shared object file: No such file or directory"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import solve_ivp\n",
        "import casadi as ca\n",
        "\n",
        "import torch\n",
        "import gpytorch\n",
        "import botorch # Mentioned for context\n",
        "import math\n",
        "\n",
        "# Optional: for nicer plots\n",
        "plt.rcParams.update({'font.size': 12, 'figure.figsize': (10, 6)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Nonlinear System and Data Generation\n",
        "\n",
        "We reuse the 1D nonlinear system from the previous GP draft:\n",
        "$$ \\frac{dx}{dt} = -x + 0.5 x^3 + u $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple 1D Nonlinear System ODE\n",
        "def nonlinear_system_ode(t, x, u_input):\n",
        "    dxdt = -x + 0.5*x**3 + u_input \n",
        "    return dxdt\n",
        "\n",
        "Ts_gp_data = 0.1 # Sampling time for data generation\n",
        "\n",
        "# Generate Training/Validation/Test Data\n",
        "N_gp_total_samples = 300 # GPs often work well with less data than ANNs\n",
        "x_gp_data_list = []\n",
        "u_gp_data_list = []\n",
        "x_next_gp_data_list = []\n",
        "\n",
        "x_current_gp = np.array([0.5]) # Initial state (1D)\n",
        "np.random.seed(123)\n",
        "\n",
        "print(\"Generating data for GP training...\")\n",
        "for i in range(N_gp_total_samples):\n",
        "    u_current_gp = np.random.uniform(-1.5, 1.5) # Random input\n",
        "    x_gp_data_list.append(x_current_gp.copy())\n",
        "    u_gp_data_list.append(u_current_gp)\n",
        "    \n",
        "    sol = solve_ivp(nonlinear_system_ode, [0, Ts_gp_data], x_current_gp, \n",
        "                      args=(u_current_gp,), method='RK45', dense_output=False)\n",
        "    x_next_gp = sol.y[:, -1]\n",
        "    x_next_gp_data_list.append(x_next_gp.copy())\n",
        "    \n",
        "    x_current_gp = x_next_gp + np.random.normal(0, 0.01) # Add process noise\n",
        "    x_current_gp = np.clip(x_current_gp, -2.5, 2.5) # Keep it bounded\n",
        "\n",
        "# --- Convert to Tensors ---\n",
        "x_gp_data_np = np.array(x_gp_data_list)\n",
        "u_gp_data_np = np.array(u_gp_data_list).reshape(-1, 1)\n",
        "x_next_gp_data_np = np.array(x_next_gp_data_list)\n",
        "\n",
        "# Inputs to GP: [x_k, u_k]\n",
        "GP_input_X_np = np.hstack((x_gp_data_np, u_gp_data_np))\n",
        "# Outputs of GP: delta_x_k = x_k+1 - x_k (Target)\n",
        "GP_delta_Y_np = x_next_gp_data_np - x_gp_data_np \n",
        "\n",
        "# Convert training data to PyTorch tensors\n",
        "train_x = torch.tensor(GP_input_X_np, dtype=torch.float32)\n",
        "train_y = torch.tensor(GP_delta_Y_np, dtype=torch.float32).flatten() # Needs to be 1D for ExactGP\n",
        "\n",
        "print(\"Data generation complete.\")\n",
        "print(f\"Train_x shape: {train_x.shape}, Train_y shape: {train_y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training the GPyTorch Model\n",
        "\n",
        "We define a model inheriting from `gpytorch.models.ExactGP`, specify a Mean and Kernel, and use the `gpytorch.mlls.ExactMarginalLogLikelihood` as the loss function to train the kernel hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. GP Model Definition\n",
        "class ExactGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        # RBF Kernel with ARD (Automatic Relevance Determination = separate lengthscale per input dim)\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
        "             gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# 2. Initialize Likelihood and Model\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "gp_torch_model = ExactGPModel(train_x, train_y, likelihood)\n",
        "\n",
        "# 3. Training Loop\n",
        "training_iter = 150\n",
        "optimizer_gp = torch.optim.Adam(gp_torch_model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_torch_model)\n",
        "\n",
        "print(\"Training GPyTorch model...\")\n",
        "gp_torch_model.train()\n",
        "likelihood.train()\n",
        "\n",
        "for i in range(training_iter):\n",
        "    optimizer_gp.zero_grad()\n",
        "    output = gp_torch_model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "    loss.backward()\n",
        "    if (i + 1) % 20 == 0:\n",
        "      print(f'Iter {i+1}/{training_iter} - Loss: {loss.item():.3f}   '\n",
        "            f'lengthscale: {gp_torch_model.covar_module.base_kernel.lengthscale.detach().numpy()}   '\n",
        "            f'noise: {likelihood.noise.item():.3f}')\n",
        "    optimizer_gp.step()\n",
        "\n",
        "print(\"GPyTorch training complete.\")\n",
        "\n",
        "# Set to evaluation mode\n",
        "gp_torch_model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Validating the Trained GP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate multi-step ahead using the GP Mean\n",
        "N_test_gpt_sim = 100\n",
        "t_test_gpt_sim = np.arange(0, N_test_gpt_sim * Ts_gp_data, Ts_gp_data)\n",
        "u_test_gpt = 0.8 * np.sin(2 * np.pi * t_test_gpt_sim / (N_test_gpt_sim * Ts_gp_data / 3)) # Test Input\n",
        "\n",
        "x_true_gpt_sim = np.zeros((N_test_gpt_sim + 1, 1))\n",
        "x_gp_mean_sim = np.zeros((N_test_gpt_sim + 1, 1))\n",
        "x_gp_upper_sim = np.zeros((N_test_gpt_sim + 1, 1))\n",
        "x_gp_lower_sim = np.zeros((N_test_gpt_sim + 1, 1))\n",
        "\n",
        "x_true_gpt_sim[0,0] = 0.2 # Initial condition\n",
        "x_gp_mean_sim[0,0] = x_true_gpt_sim[0,0]\n",
        "x_gp_lower_sim[0,0] = x_true_gpt_sim[0,0]\n",
        "x_gp_upper_sim[0,0] = x_true_gpt_sim[0,0]\n",
        "\n",
        "current_x_true_gpt = x_true_gpt_sim[0,0]\n",
        "current_x_gp_mean = x_gp_mean_sim[0,0]\n",
        "\n",
        "for k in range(N_test_gpt_sim):\n",
        "    # True system step\n",
        "    sol_true_gpt = solve_ivp(nonlinear_system_ode, [0, Ts_gp_data], [current_x_true_gpt], \n",
        "                              args=(u_test_gpt[k],), method='RK45')\n",
        "    current_x_true_gpt = sol_true_gpt.y[0, -1]\n",
        "    x_true_gpt_sim[k+1, 0] = current_x_true_gpt\n",
        "    \n",
        "    # GP prediction step (using mean for next step)\n",
        "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "        gp_input_torch = torch.tensor([[current_x_gp_mean, u_test_gpt[k]]], dtype=torch.float32)\n",
        "        observed_pred = likelihood(gp_torch_model(gp_input_torch))\n",
        "        mean_delta_x = observed_pred.mean\n",
        "        lower, upper = observed_pred.confidence_region() # 95% confidence\n",
        "    \n",
        "    current_x_gp_mean = current_x_gp_mean + mean_delta_x.item()\n",
        "    x_gp_mean_sim[k+1, 0] = current_x_gp_mean\n",
        "    # Note: These bounds are based on the one-step-ahead uncertainty.\n",
        "    # Rigorous multi-step bounds would require uncertainty propagation.\n",
        "    x_gp_lower_sim[k+1, 0] = current_x_gp_mean - (mean_delta_x.item() - lower.item())\n",
        "    x_gp_upper_sim[k+1, 0] = current_x_gp_mean + (upper.item() - mean_delta_x.item())\n",
        "\n",
        "# Plot multi-step ahead prediction comparison\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(t_test_gpt_sim, x_true_gpt_sim[:-1, 0], 'b-', linewidth=2, label='True $x(t)$')\n",
        "plt.plot(t_test_gpt_sim, x_gp_mean_sim[:-1, 0], 'r--', label='GPyTorch Mean Predicted $x(t)$')\n",
        "\n",
        "plt.fill_between(t_test_gpt_sim, \n",
        "                 x_gp_lower_sim[:-1, 0], \n",
        "                 x_gp_upper_sim[:-1, 0], \n",
        "                 color='red', alpha=0.2, label='GP 95% CI (one-step, illustrative)')\n",
        "\n",
        "plt.xlabel('Time (s)'); plt.ylabel('State $x$'); plt.grid(True); plt.legend()\n",
        "plt.title('GPyTorch Model Validation - Multi-Step Ahead Prediction')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. GP-MPC: Integration and Use of Uncertainty\n",
        "\n",
        "As with ANN-MPC, integrating a GPyTorch model into a CasADi-based NMPC solver requires overcoming the framework gap so that CasADi can get gradients.\n",
        "\n",
        "1.  **Symbolic Re-implementation:** For a kernel like RBF, the GP mean prediction formula (which involves the training data and the kernel matrix inversion) can, in principle, be implemented using CasADi's symbolic math operations. This gives CasADi full AD capability but is mathematically and technically complex to implement robustly and efficiently.\n",
        "2.  **ONNX Export:** GPyTorch models can potentially be exported to ONNX, then imported by CasADi (requires CasADi's ONNX interface).\n",
        "3.  **Using GPyTorch/BoTorch for MPC (Alternative Framework):** One could implement the MPC optimization *directly in PyTorch*, using optimizers like `torch.optim.LBFGS` and relying on PyTorch's Autograd for gradients. This avoids CasADi but requires implementing the MPC optimization steps (handling constraints, multiple steps) manually within PyTorch, which is less common for constrained control than using dedicated NLP tools like CasADi+IPOPT.\n",
        "\n",
        "### Using GP Uncertainty in MPC\n",
        "The real power of GP-MPC comes from using the variance information.\n",
        "\n",
        "*   **Robust Constraints:** Instead of just constraining the mean prediction, we can use **Chance Constraints** or constraint back-offs. A chance constraint might look like:\n",
        "   $ P(\\hat{x}_{k+j|k} \\le x_{max}) \\ge (1-\\delta) $ \n",
        "   Assuming the predictive distribution $\\mathcal{N}(\\mu_{GP}, \\sigma_{GP}^2)$ is a good approximation, this can be converted into a deterministic constraint on the mean using the inverse CDF:\n",
        "   $ \\mu_{GP}(x,u) + \\Phi^{-1}(1-\\delta) \\cdot \\sigma_{GP}(x,u) \\le x_{max} $\n",
        "   where $\\Phi^{-1}$ is the inverse CDF of a standard normal distribution (e.g., for 95% confidence, $\\Phi^{-1}(0.95) \\approx 1.645$). This makes the controller more conservative (backs off from the constraint) when the model's uncertainty is high.\n",
        "*   **Objective Function Modification:** Add terms to the cost function that penalize actions leading to predictions with high variance, steering the system towards regions of known behaviour.\n",
        "*   **Active Learning / Bayesian Optimization with BoTorch:** The GP model uncertainty can be used to guide an **acquisition function** (e.g., Expected Improvement, Upper Confidence Bound - UCB). Libraries like **BoTorch** (built on GPyTorch) are designed for this. While typically used for offline experiment design, these ideas can inform MPC by adding an 'exploration' objective to the MPC cost function to deliberately reduce model uncertainty online, balancing control performance (exploitation) with data-gathering (exploration).\n",
        "\n",
        "### Conceptual GP-MPC Simulation\n",
        "As with the ANN-MPC and for the same reasons (difficulty of true symbolic integration in a notebook), the simulation loop below will use the **known analytical model** inside the MPC controller setup. This allows us to run the simulation and discuss the concepts, but the MPC is not *using* the trained GP for its internal predictions during this simulation run. A true GP-MPC would replace the `true_model_intg` call in the CasADi formulation with a CasADi-compatible GP mean function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Conceptual GP-MPC Simulation Loop ---\n",
        "# Using the same structure as 6.2, but just running it for illustration.\n",
        "# We reuse the NMPC formulation, but conceptually, f(x,u) would be replaced by the GP mean.\n",
        "\n",
        "print(\"Running MPC using ANALYTICAL model for demonstration purposes.\")\n",
        "# ---- Reuse CasADi Setup from ANN-MPC, just rename ----\n",
        "opti_gp_concept_mpc = ca.Opti()\n",
        "X_gp_c_sym = opti_gp_concept_mpc.variable(nx_gp, Np_gp_mpc + 1)\n",
        "U_gp_c_sym = opti_gp_concept_mpc.variable(nu_gp, Np_gp_mpc)\n",
        "\n",
        "x0_gp_c_param = opti_gp_concept_mpc.parameter(nx_gp)\n",
        "u_prev_gp_c_param = opti_gp_concept_mpc.parameter(nu_gp)\n",
        "x_sp_gp_c_param = opti_gp_concept_mpc.parameter(Np_gp_mpc)\n",
        "\n",
        "obj_gp_c_mpc = 0\n",
        "for j in range(Np_gp_mpc):\n",
        "    obj_gp_c_mpc += Q_x_gpmpc * (X_gp_c_sym[0, j+1] - x_sp_gp_c_param[j])**2 \n",
        "    obj_gp_c_mpc += R_u_gpmpc * (U_gp_c_sym[0, j])**2\n",
        "    delta_u_gp_c = U_gp_c_sym[0, j] - (u_prev_gp_c_param[0] if j==0 else U_gp_c_sym[0, j-1])\n",
        "    obj_gp_c_mpc += S_u_gpmpc * delta_u_gp_c**2\n",
        "opti_gp_concept_mpc.minimize(obj_gp_c_mpc)\n",
        "\n",
        "# !!! USING TRUE MODEL FOR DYNAMICS INSIDE MPC FOR THIS SIMULATION !!!\n",
        "true_model_ode_cas_gp = ca.Function('true_ode_cas_gp', [x_s_ekf, u_s_ekf], \n",
        "                                         [nonlinear_system_ode(0, x_s_ekf, u_s_ekf)],\n",
        "                                         ['x','u'],['dx'])\n",
        "true_model_intg_gp = ca.integrator('true_intg_gp', 'rk', {'x':x_s_ekf, 'p':u_s_ekf, 'ode':true_model_ode_cas_gp(x_s_ekf, u_s_ekf)}, \n",
        "                                    {'tf': Ts_gp_mpc, 'simplify': True, 'number_of_finite_elements': 4})\n",
        "\n",
        "opti_gp_concept_mpc.subject_to(X_gp_c_sym[:,0] == x0_gp_c_param)\n",
        "for j in range(Np_gp_mpc):\n",
        "     res_dyn_gp = true_model_intg_gp(x0=X_gp_c_sym[:,j], p=U_gp_c_sym[:,j])\n",
        "     opti_gp_concept_mpc.subject_to(X_gp_c_sym[:,j+1] == res_dyn_gp['xf'])\n",
        "     # Add Constraints\n",
        "     opti_gp_concept_mpc.subject_to(opti_gp_concept_mpc.bounded(u_min_gpmpc, U_gp_c_sym[0,j], u_max_gpmpc))\n",
        "     delta_u_c_gp_c = U_gp_c_sym[0,j] - (u_prev_gp_c_param[0] if j==0 else U_gp_c_sym[0,j-1])\n",
        "     opti_gp_concept_mpc.subject_to(opti_gp_concept_mpc.bounded(-delta_u_max_gpmpc, delta_u_c_gp_c, delta_u_max_gpmpc))\n",
        "     opti_gp_concept_mpc.subject_to(opti_gp_concept_mpc.bounded(x_min_gpmpc, X_gp_c_sym[0,j+1], x_max_gpmpc))\n",
        "\n",
        "opti_gp_concept_mpc.solver('ipopt', opts_gp_mpc)\n",
        "\n",
        "# --- Run Simulation Loop ---\n",
        "sim_time_gp_mpc_loop = 20 # s\n",
        "num_sim_steps_gp_loop = int(sim_time_gp_mpc_loop / Ts_gp_mpc)\n",
        "x_plant_gp_loop_current = np.array([0.2]) \n",
        "u_plant_gp_loop_prev = np.array([0.0]) \n",
        "x_sp_horizon_gp_loop = np.full(Np_gp_mpc, x_sp_target_gpmpc)\n",
        "\n",
        "t_log_gp_loop = np.zeros(num_sim_steps_gp_loop + 1)\n",
        "X_log_gp_loop_plant = np.zeros((nx_gp, num_sim_steps_gp_loop + 1))\n",
        "U_log_gp_loop = np.zeros((nu_gp, num_sim_steps_gp_loop))\n",
        "X_log_gp_loop_plant[:, 0] = x_plant_gp_loop_current\n",
        "\n",
        "U_guess_gp_loop = np.full((nu_gp, Np_gp_mpc), 0.0)\n",
        "X_guess_gp_loop = np.tile(x_plant_gp_loop_current.reshape(nx_gp,1), (1, Np_gp_mpc + 1))\n",
        "\n",
        "# (Simulation loop code structure similar to ANN-MPC in 6.2)\n",
        "# ... Solve, Apply, Simulate Plant, Log ...\n",
        "print(\"Executing conceptual MPC simulation (using analytical model inside MPC).\")\n",
        "print(\"See results from GP Training above for how the data-driven model itself behaves.\")\n",
        "# NOTE: Skipping full re-implementation of sim loop here for brevity,\n",
        "# as it would look identical to the ANN-MPC one but calling opti_gp_concept_mpc.solve()\n",
        "# The key part of THIS notebook is the GP TRAINING and the DISCUSSION."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key Takeaways\n",
        "\n",
        "*   Gaussian Processes, implemented here using GPyTorch, provide a Bayesian approach to learning nonlinear dynamics from data.\n",
        "*   A key strength is the ability to quantify model uncertainty (predictive variance) alongside the mean prediction.\n",
        "*   Hyperparameters for the GP's kernel are efficiently trained by maximizing the marginal log likelihood.\n",
        "*   Integrating a GP model symbolically into a CasADi-based NMPC for gradient-based optimization is challenging, often requiring symbolic re-implementation of the GP mean function or advanced interfacing (e.g., ONNX).\n",
        "*   GP uncertainty enables robust MPC formulations (e.g., using chance constraints) and can be used to guide active learning (e.g., using BoTorch acquisition functions) to improve the model efficiently.\n",
        "*   Computational cost for training and prediction ($O(N^3), O(N^2)$) for standard GPs necessitates sparse GP approximations for larger problems.\n",
        "\n",
        "This concludes our overview of data-driven models for MPC. We have seen how ANNs, PINNs, and GPs can serve as the predictive engine, each with a unique set of capabilities, strengths, and implementation challenges."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
